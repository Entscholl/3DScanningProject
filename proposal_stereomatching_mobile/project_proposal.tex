\documentclass[a4paper,pagesize 10pt]{scrartcl}

\usepackage{graphicx}
\usepackage{scalefnt}
\usepackage{textfit}
\usepackage{url}

\begin{document}


\begin{center}{\Huge\textbf{Project Proposal}}\end{center}

\begin{center}{\Large\textbf{Stereoscopic Reconstruction on Android}}\end{center}


\section{Abstract}

%
%Write a short abstract of your planned project.
%
%Cite papers that you want to use as references (e.g. Adelson et al.~\cite{adelson1984pyramid}).
%
%Include an overview figure that shows your planned processing pipeline.
%
%
The problem of feature detection is well known and studied, but can be computationally expensive. In this project, we aim to build a robust feature detection system which will be more accessible, by utilizing a mobile phones built in hardware. Inspired by Android 9 (Pie)
%next version right now would be Android Q (10)
providing an API for multiple cameras ~\cite{AndroidPMultiCamera} doing true stereo matching on mobile might become much easier. Currently, most stereo matching algorithms take two pictures sequentially and rely on the accelerometer and gyroscope to detect the positional disparity. Finally, we demonstrate the usability of our feature matching by applying a bokeh effect to one of the input images.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{Pipeline.PNG}
	\caption{Our Proposed Processing Pipeline}
\end{figure}

% Camera calibration for intrinsic parameters necessary, e.g. OpenCV

\paragraph{Data Capture}
First, we capture two distinct RGB images from the phones camera sensor. At the same time, we also capture the orientation of the camera with the gyroscope sensor. Starting with the first image, we sample the accelerometer to obtain the inertia at discrete points between the two pictures.
%\paragraph{Epipolar Geometry}
%First, we detect and match features between the two RGB images to obtain the epipolar geometry.[CITATION NEEDED]
%Still not sure how that is exactely done
%Sparse feature matching maybe? e.g. SIFT, SURF, ORB
%I don't know if this actually works. The slides are kind of ominous.
\paragraph{Process Sensor Data}
The inertial data of the accelerometer combined is integrated twice to obtain the positional difference~\cite{Seifert2007,Kok2017}. Here, the numerical integration error accumulates over time. In contrast, the rotational difference between to RGB data sets is obtained by sampling the gyroscope sensor. We will test if the supplied commodity sensors are accurate enough to augment the image rectification process.
\paragraph{Feature Matching and Optimization}
After detecting and matching features we then use the methods from the lecture to optimize the transformation with a non-linear solver e.g. Ceres. This step is optional depending on the quality of the sensor data.
\paragraph{Image Rectification}
With the calculated translation and rotation, we can rectify the images~\cite{loop1999, Fusiello2000}. 
\paragraph{Stereo Matching}
Once rectified, we can apply a stereo matching algorithm e.g. FLANN, Block Matching, PatchMatch ~\cite{Bleyer2011} or PM-Huber ~\cite{Heise2013} to calculate the displacement map. 
\paragraph{Depth Map}
The depth map is then calculated by triangulation as discussed in the lecture.
%it's described in the lecture slides and on stack overflow that way. Have to find a paper though.
\paragraph{Application}
We then use the obtained depth map to create an artificial Bokeh effect, by applying a blur effect to all everything closer or farther than an arbitrary chosen depth.


With this approach, the accuracy of the depth estimation depends highly on the accuracy, numerical error and sampling frequency of the sensors.
%Fallback would be basically Bundle Adjustment, which I don't really want to do.

The targeted platform is Android and the technical implementation will be done using java and C++ with the aid of the library OpenCV for various tasks, such as camera calibration or feature detection/matching.
%Possibly OpenMP or OpenGL (Compute Shaders) for acceleration, depends if performance sucks or not

%
%
%Figure: /----> Picture A, Gyrodata A => Epipolar Lines => Rectify \
%Data Capture--------------------------------------->Inertial Data => Stereo Matching=> Depth => Bokeh	
%		 \___-> Picture B, Gyrodata B => Epipolar Lines => Rectify /
%	 	  \			     	 ||
%		   \___->     Sensor Data B
\section{Milestones}
\textbf{Week 24}: Official project start: Project setup\newline
\textbf{Week 25}: Sensor Data Processing and Image Rectification\newline
\textbf{Week 26}: Stereo Matching\newline
\textbf{Week 27}: Bokeh effect\newline
\textbf{Week 28}: Feature Matching/Optimization \newline
\textbf{Week 29}: Testing\newline
\textbf{Week 30}: Create Poster\newline
\section{Requirements}
Preferably, a multi-camera smartphone which offers an API for that would be good  (LOGICAL\_MULTI\_CAMERA\footnote{Not all phones with Android 9 and stereo cameras actually support this: https://developer.android.com/reference/android/hardware/camera2/CameraCharacteristics}) , but we are able to adapt if that is not possible. Alternatively, an USB camera which is supported by android\footnote{https://source.android.com/devices/camera/external-usb-cameras} could be used to do traditional stereo vision. A Tango smartphone would be nice to benchmark our system against a full mobile depth camera.
\section{Team}
\begin{itemize}
	\item Iulia - Otilia Mustea
	\item Manuel Schwonberg
	\item Jeremias Neth
	\item Michael Wechner
\end{itemize}


% references
{\small
	\bibliographystyle{plain}
	\bibliography{project_proposal}
}

\end{document}


