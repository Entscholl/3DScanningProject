\documentclass[a4paper,pagesize 10pt]{scrartcl}

\usepackage{graphicx}
\usepackage{scalefnt}
\usepackage{textfit}
\usepackage{url}

\begin{document}


\begin{center}{\Huge\textbf{Project Proposal}}\end{center}
\begin{center}{\Large\textbf{''Stereoscopic reconstruction on Android''}}\end{center}

\section{Abstract}

%
%Write a short abstract of your planned project.
%
%Cite papers that you want to use as references (e.g. Adelson et al.~\cite{adelson1984pyramid}).
%
%Include an overview figure that shows your planned processing pipeline.
%
%
The problem of feature detection is well known and studied, but can be computationally expensive. In this project, we aim to build a robust feature detection system which will be more accessible, by utilizing a mobile phones built in hardware. Inspired by the next version of Android providing an API for muliple cameras ~\cite{AndroidPMultiCamera} doing true stereo matching on mobile might become much easier. Currently, most stereo matching algorithms take two pictures sequentially and rely on the accelerometer and other positonal sensors to detect the positional disparity. Finally, we demonstrate the usabilty of our feature matching by applying a bokeh effect to one of the input images.
\begin{figure}[h]
	\centering
	\includegraphics[scale=0.55]{Pipeline.PNG}
	\caption{Our Proposed Processing Pipeline}
\end{figure}


% Camera calibration for intrinsic parameters necessary, e.g. OpenCV

First, we detect and match features between the two RGB images to obtain the epipolar geometry.[CITATION NEEDED]
%Still not sure how that is exactely done
%Sparse feature matching maybe? e.g. SIFT, SURF, ORB
Once we have the epipolar geometry, we can rectify the images ~\cite{loop1999}. Once rectified we can apply stereo matching algorithms e.g. PatchMatch ~\cite{Bleyer2011} or PM-Huber ~\cite{Heise2013} to calculate the displacement map. In the optimal parallel case we can calculate the depth via triangulation [CITATION NEEDED]. 
%it's described in the lecture slides and on stack overflow that way. Have to find a paper though.

The inertial data of the accelerometer is integrated twice to obtain the positional difference~\cite{Seifert2007}. Here, the numerical integration error accumulates over time. In contrast, the rotational difference between to RGB data sets is obtained by sampling the gyroscope sensor. We test if the supplied commodity sensors are accurate enough to augment the image rectification.

With this depth information, we are able to apply advanced image processing algorithms e. g. a Bokeh effect ~\cite{Lee2008}.

%\begin{figure}[h]
%	\centering
%	\includegraphics[width=\linewidth]{overview}
%	\caption{Method overview.}
%	\label{fig:overview}
%\end{figure}
%								Sensor Data	A
%									 ||
%Figure: Picture A => Epipolar Lines => Rectify \
%												 => Disparity Map => Depth => Bokeh	
%		 Picture B => Epipolar Lines => Rectify /
%									 ||
%								Sensor Data B

\section{Requirements}
Perferably, a multi-camera smartphone with android 9 support would be good, but we are able to adapt if that is not possible. A Tango smartphone would be nice to benchmark our system against a full mobile depth camera.
\section{Team}
\begin{itemize}
	\item Iulia - Otilia Mustea
	\item Manuel Schwonberger
	\item Jeremias Neth
	\item Michael Wechner
\end{itemize}


% references
{\small
	\bibliographystyle{plain}
	\bibliography{project_proposal}
}

\end{document}


